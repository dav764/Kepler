import json
import pandas as pd
from bs4 import BeautifulSoup
import re
import os

def update_kepler_html(html_path, csv_path, output_path, dataset_label):
    """
    Met √† jour un fichier HTML Kepler.gl avec de nouvelles donn√©es CSV.
    """
    try:
        # --- 1. Lecture du fichier CSV ---
        print(f"Lecture du fichier CSV : {csv_path}")
        df = pd.read_csv(csv_path, encoding='latin-1') # Utilise la virgule par d√©faut

        # Conversion du DataFrame pour Kepler.gl
        new_all_data = [df.columns.tolist()] + df.values.tolist()
        new_fields = []
        for col in df.columns:
            if pd.api.types.is_numeric_dtype(df[col]):
                field_type = 'integer' if pd.api.types.is_integer_dtype(df[col]) else 'real'
            else:
                field_type = 'string'
            new_fields.append({'name': col, 'type': field_type})
        print(f"Donn√©es CSV lues et converties : {len(new_all_data) - 1} lignes.")

        # --- 2. Lecture du fichier HTML ---
        print(f"Lecture du fichier HTML : {html_path}")
        with open(html_path, 'r', encoding='utf-8') as f:
            soup = BeautifulSoup(f, 'html.parser')

        # --- 3. Extraction des donn√©es existantes ---
        script_tag = soup.find('script', string=re.compile(r'function customize\(keplerGl, store\)'))
        if not script_tag:
            raise ValueError("Impossible de trouver le bloc <script> de configuration dans le HTML.")
        
        script_content = script_tag.string
        datasets_match = re.search(r'const datasets = (\[.*?\]);', script_content, re.DOTALL)
        if not datasets_match:
            raise ValueError("Impossible d'extraire la variable 'datasets' du HTML.")
        
        datasets_str = datasets_match.group(1)
        map_datasets = json.loads(datasets_str)
        print("Donn√©es existantes extraites du HTML.")

        # --- 4. Remplacement du dataset CSV ---
        csv_found = False
        # --- LIGNE DE D√âBOGAGE AJOUT√âE ---
        print(f"\n-> Je cherche le dataset avec le label EXACT : '{dataset_label}'")
        
        for dataset in map_datasets:
            label = dataset.get('info', {}).get('label') or dataset.get('data', {}).get('label')
            # --- LIGNE DE D√âBOGAGE AJOUT√âE ---
            print(f"  - Je v√©rifie le label trouv√© dans le HTML : '{label}'")
            
            if label == dataset_label:
                dataset['data']['allData'] = new_all_data
                dataset['data']['fields'] = new_fields
                csv_found = True
                break
        
        # --- LIGNES DE D√âBOGAGE AJOUT√âE ---
        if csv_found:
            print("--> SUCC√àS : Le dataset a √©t√© trouv√© et remplac√© !")
        else:
            print("--> ATTENTION : Le dataset n'a PAS √©t√© trouv√©. Aucune modification ne sera effectu√©e.")

        # --- 5. R√©injection des donn√©es mises √† jour ---
        updated_datasets_str = json.dumps(map_datasets, separators=(',', ':'))
        updated_script_content = script_content.replace(datasets_str, updated_datasets_str, 1)
        script_tag.string.replace_with(updated_script_content)

        # --- 6. Sauvegarde du nouveau fichier HTML ---
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(str(soup))
        
        print(f"\nüéâ Succ√®s ! Fichier mis √† jour sauvegard√© ici : {output_path}")

    except FileNotFoundError as e:
        print(f"\nERREUR : Fichier non trouv√© - {e}")
    except Exception as e:
        print(f"\nERREUR : Une erreur est survenue - {e}")

# --- POINT D'ENTR√âE DU SCRIPT ---
if __name__ == '__main__':
    HTML_SOURCE_PATH = 'index.html'
    CSV_SOURCE_PATH = 'flow_final.csv'
    HTML_OUTPUT_PATH = 'index.html'

    dataset_label_to_find = os.path.basename(CSV_SOURCE_PATH)
    
    update_kepler_html(HTML_SOURCE_PATH, CSV_SOURCE_PATH, HTML_OUTPUT_PATH, dataset_label_to_find)
